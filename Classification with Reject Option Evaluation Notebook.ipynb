{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c22115a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7fbcc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:40:25.556230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "### -------- Load libraries ------- ###\n",
    "# Load Huggingface transformers\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizer\n",
    "from transformers import TFAlbertModel,  AlbertConfig, AlbertTokenizer\n",
    "from transformers import TFRobertaModel,  RobertaConfig, RobertaTokenizer\n",
    "from transformers import TFDistilBertModel, BertTokenizer, DistilBertConfig\n",
    "from transformers import TFXLMModel, XLMTokenizer, XLMConfig, TFSequenceSummary\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "# Then what you need from tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import official.nlp.modeling.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "# And pandas for data import + sklearn because you allways need sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "#loggging\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#uncertainty\n",
    "from robustness_metrics.metrics import uncertainty\n",
    "\n",
    "#This metric computes the percentage ofcorrectly rejected examples, which is the percentage \n",
    "#of incorrect predictionsamong all the abstained examples.\n",
    "from metrics import AbstainPrecision\n",
    "''' Different from `AbstainPrecision`, `AbstainRecall` computes the percentage of\n",
    "  correctly abstained examples among all the incorrect predictions that **could\n",
    "  have been abstained**. '''\n",
    "from metrics import AbstainRecall\n",
    "\n",
    "#custom\n",
    "#custom function\n",
    "from Refactoring.loading import load, load_combined_languages, load_artificial_ood, get_reduced_label_dict, get_train_dict\n",
    "from Refactoring.model import load_model, build_classifier_model, build_classifier_model_last4\n",
    "from Refactoring.model_train_param import build_classifier_model_training, build_classifier_model_last4_training #includes training paramter\n",
    "from Refactoring.model import bert_optimizer, get_layers, dlr_optimizer\n",
    "from Refactoring.evaluation import test_prediction, save_report, save_metrics\n",
    "\n",
    "from Refactoring.uncertainty_functions import create_Vanilla_pred, mc_dropout_sampling, mc_predictions, save_single_model_predictions, create_deep_ensemble_predictions, compute_metrics,compute_metrics_zero_shot,get_class_ranking,abstain_accuracy_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c376a",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85cf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "######### Necessary Setting s########\n",
    "\n",
    "method = \"Deep Ensemble\" #Deep Ensemble, MC Dropout, Vanilla\n",
    "\n",
    "model_path = \"EN_last4_concat_cls_1_multi_False_Distilbert_0.0001_5_20220107-091042_random_state42\" #for MC Dropout & Vanilla\n",
    "\n",
    "train_part2_ood = True #set to true if one wants to evaluate for artificial ood setting --> additional mapping after predicting needed\n",
    "\n",
    "zero_shot = False\n",
    "\n",
    "\n",
    "assert(train_part2_ood + zero_shot <2) #either artificial ood or zero-shot setting (or none, i.e. in-domain)\n",
    "\n",
    "assert(method==\"Vanilla\" or method==\"Deep Ensemble\" or method==\"MC Dropout\")\n",
    "\n",
    "\n",
    "#####################################\n",
    "########## Seeds ####################\n",
    "\n",
    "seed= 42 #42 standard, 0, 21, 99, 365\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "######### Necessary Settings ########\n",
    "\n",
    "language = \"EN\" #EN, DE, RO, All languages only choice if standard bert_type\n",
    "\n",
    "test_language = \"DE\" #EN, DE, RO, All languages only choice if standard bert_type\n",
    "\n",
    "language_model_relation = \"specific\" # specific vs multi\n",
    "\n",
    "bert_type = \"Distilbert\"  #Roberta, #Distilbert, #BERT # RobertaXLM\n",
    "\n",
    "epochs = 5 #5 is standard for bert models , 10 for roberta models\n",
    "\n",
    "learning_rate = 1e-4 #5e-5 or 2e-5, 1e-4, 4e-4\n",
    "\n",
    "layer_strategy = 'last4' #last, last4 --> automatically defaults to last2 for distilbert models\n",
    "\n",
    "reduce_strategy = 'cls' # cls,mean, max\n",
    "\n",
    "last_layer_strategy = \"concat\" #mean, max, concat --> only relevant for last4 layer strategy\n",
    "\n",
    "decay_factor = 1 # only takes effect with layer wise lr\n",
    "\n",
    "layer_wise_lr = False\n",
    "\n",
    "\n",
    "if zero_shot:\n",
    "    assert(language_model_relation == \"multi\")\n",
    "    assert(language != test_language)\n",
    "\n",
    "#####################################\n",
    "#########  Default Settings  ########\n",
    "\n",
    "#these were not changed in the end. sometimes still used for logging/naming purposes\n",
    "\n",
    "size = \"medium\" #used only medium in the end\n",
    "\n",
    "cased = False # always used recommended case for each model, thus do not change\n",
    "\n",
    "class_weighting = False #relic of initial experiments\n",
    "\n",
    "max_length = 40 #only 40 used\n",
    "\n",
    "batch_size = 32 #only 32 used\n",
    "\n",
    "val_size=0.2 #only 0.2 used\n",
    "\n",
    "num_ensemble = 1 #only used for logging in with Vanilla method\n",
    "\n",
    "freeze = False #if one wants to train classification head first only. placeholder is in training loop, \n",
    "                #but should be adjusted if one wants to use it \n",
    "                #(e.g. set different learning rates for the two training phases)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69804f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tensorflow.python.client import device_lib\\nprint(device_lib.list_local_devices())\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf176c",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb68fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3907593/2786153072.py:2: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  X_train, X_test, y_train, y_test, n_classes_test, label_dict_test = load(language=language, seed=seed)\n",
      "/tmp/ipykernel_3907593/2786153072.py:4: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  label_dict_train, n_classes_reduced = get_reduced_label_dict(language=language)\n"
     ]
    }
   ],
   "source": [
    "if not zero_shot:\n",
    "    X_train, X_test, y_train, y_test, n_classes_test, label_dict_test = load(language=language, seed=seed)\n",
    "    if train_part2_ood:\n",
    "        label_dict_train, n_classes_reduced = get_reduced_label_dict(language=language)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test, n_classes_test, label_dict_test = load(language=test_language, seed=seed)\n",
    "    label_dict_train, n_classes_train = get_train_dict(language=language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284ca571",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b089f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output hidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "2022-01-07 14:40:44.118421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-07 14:40:44.121221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-01-07 14:40:44.121246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:40:44.124758: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-07 14:40:44.128489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-07 14:40:44.130353: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-07 14:40:44.134219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-07 14:40:44.136862: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-07 14:40:44.143970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-07 14:40:44.145735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-01-07 14:40:44.146333: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 14:40:44.167455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3850000000 Hz\n",
      "2022-01-07 14:40:44.168651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e578b566c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-07 14:40:44.168696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-01-07 14:40:44.237700: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e57a9610a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-07 14:40:44.237737: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2022-01-07 14:40:44.238750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-01-07 14:40:44.238801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:40:44.238839: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-07 14:40:44.238858: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-07 14:40:44.238878: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-07 14:40:44.238897: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-07 14:40:44.238916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-07 14:40:44.238935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-07 14:40:44.240340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-01-07 14:40:44.240387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:40:44.671147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-07 14:40:44.671174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2022-01-07 14:40:44.671179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2022-01-07 14:40:44.671910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9511 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\n",
      "2022-01-07 14:40:44.719641: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-07 14:40:44.758678: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel object at 0x7f070b52bd00>\n"
     ]
    }
   ],
   "source": [
    "tokenizer, transformer_model, config = load_model(layer_strategy=layer_strategy, bert_type=bert_type, cased=cased, language=language, language_model_relation=language_model_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4084c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### ----- Evaluate the model ------ ###\n",
    "# Ready test data\n",
    "y_test_categorical = to_categorical(y_test,num_classes=n_classes_test)\n",
    "test_x = tokenizer(\n",
    "    text=list(X_test),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d42ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"Vanilla\":\n",
    "    if train_part2_ood:\n",
    "        print(\"Model for Vanilla & Out-of-Domain\") \n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_reduced)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_reduced)\n",
    "    elif zero_shot:\n",
    "        print(\"Model for Vanilla & Zero-Shot\")         \n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_train)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_train)    \n",
    "    else:\n",
    "        print(\"Model for Vanilla & In-Domain\")  \n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_test)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_test)\n",
    "    model.summary()\n",
    "elif method == \"MC Dropout\":\n",
    "    if train_part2_ood:\n",
    "        print(\"Model for MC Dropout & Out-of-Domain\")\n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_reduced)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_reduced)\n",
    "    elif zero_shot:\n",
    "        print(\"Model for MC Dropout & Zero-Shot\")   \n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_train)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_train)\n",
    "    else:\n",
    "        print(\"Model for MC Dropout & In-Domain\")\n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes_test)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4_training(training=True, seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes_test)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff8405",
   "metadata": {},
   "source": [
    "# Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ec9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"Vanilla\":\n",
    "    if train_part2_ood:\n",
    "        print(\"mapping predictions --> ood\")\n",
    "        predictions_probs = create_Vanilla_pred(model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "        compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=predictions_probs,dropout_probs_mean=predictions_probs, dropout_probs_var=predictions_probs,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "    elif zero_shot:\n",
    "        print(\"mapping predictions --> zero_shot\")\n",
    "        df_dict = {}\n",
    "        predictions_probs = create_Vanilla_pred(model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "        for k in [1,3,5,10,20,30,40,50,60,70,80,90,100]:\n",
    "            df = compute_metrics_zero_shot(method=method,k=k, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=predictions_probs,dropout_probs_mean=predictions_probs, dropout_probs_var=predictions_probs,num_ensemble=1, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "            df_dict[\"combined\"+\"_\"  + str(k) +\"_\" + model_path + \"_\" + str(language)+\"_\"+str(layer_strategy) + \"_\"+str(last_layer_strategy)+\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor) +\"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed) +\"_technique_\" +str(method)+\"_ensembles_\"+str(num_ensemble)+\".csv\"] = df        \n",
    "        for key, value in df_dict.items():\n",
    "            value.to_csv(key)\n",
    "    else:\n",
    "        print(\"NOT mapping predictions --> in-domain\")\n",
    "        predictions_probs = create_Vanilla_pred(model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot)\n",
    "        compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=predictions_probs,dropout_probs_mean=predictions_probs, dropout_probs_var=predictions_probs,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46116463",
   "metadata": {},
   "source": [
    "# MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddeb697f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if method == \"MC Dropout\":\n",
    "    if train_part2_ood:\n",
    "        print(\"mapping predictions --> ood\")\n",
    "        for num_ensemble in [3,5,10,20,30,40,50,60,70,80,90,100]:#\n",
    "            print(num_ensemble)\n",
    "            dropout_probs_samples, dropout_probs_mean, dropout_probs_var = mc_predictions(num_ensemble=num_ensemble, model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "\n",
    "            compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=dropout_probs_samples,dropout_probs_mean=dropout_probs_mean, dropout_probs_var=dropout_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "    elif zero_shot:\n",
    "        print(\"mapping predictions --> zero-shot\")\n",
    "        df_dict = {}\n",
    "        for num_ensemble in [3,5,10,20,30,40,50,60,70,80,90,100]:#\n",
    "            dropout_probs_samples, dropout_probs_mean, dropout_probs_var = mc_predictions(num_ensemble=num_ensemble, model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "            for k in [1,3,5,10,20,30,40,50,60,70,80,90,100]:\n",
    "                df = compute_metrics_zero_shot(method=method,k=k, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=dropout_probs_samples,dropout_probs_mean=dropout_probs_mean, dropout_probs_var=dropout_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "                df_dict[\"combined\"+\"_\"  + str(k) +\"_\" + model_path + \"_\" + str(language)+\"_\"+str(layer_strategy) + \"_\"+str(last_layer_strategy)+\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor) +\"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed) +\"_technique_\" +str(method)+\"_ensembles_\"+str(num_ensemble)+\".csv\"] = df\n",
    "        for key, value in df_dict.items():\n",
    "            value.to_csv(key)\n",
    "    else:\n",
    "        print(\"NOT mapping predictions --> in-domain\")\n",
    "        for num_ensemble in [3,5,10,20,30,40,50,60,70,80,90,100]:#\n",
    "            print(num_ensemble)\n",
    "            dropout_probs_samples, dropout_probs_mean, dropout_probs_var = mc_predictions(num_ensemble=num_ensemble, model_path=model_path, test_x=test_x, model=model, ood=train_part2_ood,zero_shot=zero_shot)\n",
    "\n",
    "            compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=dropout_probs_samples,dropout_probs_mean=dropout_probs_mean, dropout_probs_var=dropout_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853b443",
   "metadata": {},
   "source": [
    "# Deep Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c0a604",
   "metadata": {},
   "source": [
    "## First execute the following cell to obtain deep ensemble predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a47a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Deep Ensemble & OOD\n",
      "EN_last4_concat_cls_1_specific_False_Distilbert_0.0001_5_20220104-171641_random_state42\n",
      "EN_last4_concat_cls_1_specific_False_Distilbert_0.0001_5_20220104-185410_random_state42\n"
     ]
    }
   ],
   "source": [
    "if method == \"Deep Ensemble\":\n",
    "    if train_part2_ood:\n",
    "        print(\"Model for Deep Ensemble & OOD\")   \n",
    "        save_single_model_predictions(test_x=test_x,tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy, config=config,language=language,language_model_relation=language_model_relation,cased=cased,learning_rate=learning_rate,seed=seed,decay_factor=decay_factor,num_ensemble=num_ensemble, n_classes=n_classes_reduced)\n",
    "    elif zero_shot:\n",
    "        print(\"Model for Deep Ensemble & Zero-Shot\")   \n",
    "        save_single_model_predictions(test_x=test_x,tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy, config=config,language=language,language_model_relation=language_model_relation,cased=cased,learning_rate=learning_rate,seed=seed,decay_factor=decay_factor,num_ensemble=num_ensemble, n_classes=n_classes_train)\n",
    "    else:\n",
    "        print(\"Model for Deep Ensemble & In-Domain\")   \n",
    "        save_single_model_predictions(test_x=test_x,tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy, config=config,language=language,language_model_relation=language_model_relation,cased=cased,learning_rate=learning_rate,seed=seed,decay_factor=decay_factor,num_ensemble=num_ensemble, n_classes=n_classes_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76be85",
   "metadata": {},
   "source": [
    "## Then after saving the predictions, load them and compute the uncertainty metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a026e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping predictions --> zero-shot\n",
      "k: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/jdeke/anaconda3/envs/yourenv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/homes/jdeke/anaconda3/envs/yourenv/lib/python3.8/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3\n",
      "k: 5\n",
      "k: 10\n",
      "k: 20\n",
      "k: 30\n",
      "k: 40\n",
      "k: 50\n"
     ]
    }
   ],
   "source": [
    "load_path=\"EN_last4_concat_cls_1_multi_False_Distilbert_0.0001_20220105-100906_random_state42_technique_For Deep Ensemble_ensembles_1.npy\"\n",
    "if method == \"Deep Ensemble\":\n",
    "    if train_part2_ood:\n",
    "        print(\"mapping predictions --> ood\")\n",
    "        for num_ensemble in [3,4,5,6,7,8,9,10]:#3,4,5,6,7,8,9,\n",
    "            print(num_ensemble)\n",
    "            model_prob_samples, model_probs_mean, model_probs_var = create_deep_ensemble_predictions(load_path=load_path,num_ensemble=num_ensemble, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "            compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=model_prob_samples,dropout_probs_mean=model_probs_mean, dropout_probs_var=model_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "    elif zero_shot:\n",
    "        print(\"mapping predictions --> zero-shot\")\n",
    "        df_dict = {}\n",
    "        for num_ensemble in [3,4,5,6,7,8,9,10]:#3,4,5,6,7,8,9,\n",
    "            model_prob_samples, model_probs_mean, model_probs_var = create_deep_ensemble_predictions(load_path=load_path,num_ensemble=num_ensemble, ood=train_part2_ood,zero_shot=zero_shot, label_dict_train=label_dict_train, label_dict_test=label_dict_test)\n",
    "            for k in [1,3,5,10,20,30,40,50,60,70,80,90,100]:\n",
    "                df = compute_metrics_zero_shot(method=method,k=k, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=model_prob_samples,dropout_probs_mean=model_probs_mean, dropout_probs_var=model_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)\n",
    "                df_dict[\"combined\"+\"_\"  + str(k) +\"_\" + model_path + \"_\" + str(language)+\"_\"+str(layer_strategy) + \"_\"+str(last_layer_strategy)+\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor) +\"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed) +\"_technique_\" +str(method)+\"_ensembles_\"+str(num_ensemble)+\".csv\"] = df\n",
    "        for key, value in df_dict.items():\n",
    "            value.to_csv(key)\n",
    "    else:\n",
    "        print(\"NOT mapping predictions --> in-domain\")\n",
    "        for num_ensemble in [3,4,5,6,7,8,9,10]:#3,4,5,6,7,8,9,\n",
    "            print(num_ensemble)\n",
    "            model_prob_samples, model_probs_mean, model_probs_var = create_deep_ensemble_predictions(load_path=load_path,num_ensemble=num_ensemble, ood=train_part2_ood,zero_shot=zero_shot)\n",
    "\n",
    "            compute_metrics(method=method, y_test=y_test, y_test_categorical=y_test_categorical,dropout_probs_samples=model_prob_samples,dropout_probs_mean=model_probs_mean, dropout_probs_var=model_probs_var,num_ensemble=num_ensemble, language=language, layer_strategy=layer_strategy, last_layer_strategy=last_layer_strategy,reduce_strategy=reduce_strategy,decay_factor=decay_factor,language_model_relation=language_model_relation,cased=cased,bert_type=bert_type,learning_rate=learning_rate,seed=seed,model_path=model_path)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372c4da",
   "metadata": {},
   "source": [
    "# In case of OOM errors with MC Dropout,  the following may help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0daba3",
   "metadata": {},
   "source": [
    "restarting kernel after, say, 70 dropout passes, and then running the large number of dropout passes not in a loop but one after one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f30923",
   "metadata": {},
   "source": [
    "saving the predictions of >70 dropout passes similar to the procedure in the deep ensemble case and then loading them again after restarting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Monte Carlo dropout inference.\n",
    "dropout_logit_samples = [mc_dropout_sampling(test_x) for _ in range(num_ensemble)]\n",
    "dropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1) for dropout_logits in dropout_logit_samples]\n",
    "dropout_prob_samples = tf.stack([dropout_prob_samples])[0]\n",
    "\n",
    "np.save(str(num_ensemble) +\" dropout ensembles samples\", dropout_prob_samples)\n",
    "\n",
    "\n",
    "dropout_prob_samples = np.load(str(num_ensemble) + \" dropout ensembles samples.npy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e81e3",
   "metadata": {},
   "source": [
    "splitting variance computation (if variance computation is the bottleneck):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ea7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dropout_probs_var_1 = tf.math.reduce_variance(dropout_prob_samples[:,:,:60], axis=0)\n",
    "dropout_probs_var_2 = tf.math.reduce_variance(dropout_prob_samples[:,:,60:120], axis=0)\n",
    "dropout_probs_var_3 = tf.math.reduce_variance(dropout_prob_samples[:,:,120:180], axis=0)\n",
    "dropout_probs_var_4 = tf.math.reduce_variance(dropout_prob_samples[:,:,180:], axis=0)\n",
    "\n",
    "\n",
    "dropout_probs_var = np.concatenate((dropout_probs_var_1,dropout_probs_var_2,dropout_probs_var_3,dropout_probs_var_4), axis=1)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yourenv]",
   "language": "python",
   "name": "conda-env-yourenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
