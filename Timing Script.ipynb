{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f85cf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### -------- Load libraries ------- ###\n",
    "# Load Huggingface transformers\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizer\n",
    "from transformers import TFAlbertModel,  AlbertConfig, AlbertTokenizer\n",
    "from transformers import TFRobertaModel,  RobertaConfig, RobertaTokenizer\n",
    "from transformers import TFDistilBertModel, BertTokenizer, DistilBertConfig\n",
    "from transformers import TFXLMModel, XLMTokenizer, XLMConfig, TFSequenceSummary\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "# Then what you need from tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import official.nlp.modeling.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "# And pandas for data import + sklearn because you allways need sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "#loggging\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#uncertainty\n",
    "from robustness_metrics.metrics import uncertainty\n",
    "\n",
    "\n",
    "\n",
    "seed= 99 #42 standard, 0, 21, 99, 365\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##settings##\n",
    "language = \"EN\" #EN, DE, RO, All languages only choice if standard bert_type\n",
    "\n",
    "language_model_relation = \"multi\" # specific vs multi\n",
    "\n",
    "size = \"medium\" #small, medium, large for roberta and albert maybe\n",
    "\n",
    "bert_type = \"Distilbert\"  #Roberta, #Distilbert, #BERT # RobertaXLM\n",
    "\n",
    "cased = False # True always cased for multilanguage\n",
    "\n",
    "class_weighting = False\n",
    "\n",
    "max_length = 40\n",
    "\n",
    "epochs = 5 #5 is standard, 10 for roberta\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 1e-4 #5e-5 or 2e-5, 1e-4, 4e-4\n",
    "\n",
    "\n",
    "layer_strategy = 'last4' #last, last4\n",
    "\n",
    "reduce_strategy = 'cls' # cls,mean, max\n",
    "\n",
    "last_layer_strategy = \"concat\" #mean, max, concat --> only relevant for last4 layer strategy\n",
    "\n",
    "\n",
    "decay_factor = 1 # only takes effect with layer wise lr\n",
    "\n",
    "layer_wise_lr = False\n",
    "\n",
    "val_size=0.2\n",
    "\n",
    "freeze = False\n",
    "\n",
    "training = True\n",
    "\n",
    "num_ensemble = 10\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69804f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12119281466083229456\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16194232736936346990\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8578424813065476343\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10468833408\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3717588638101577566\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 09:29:44.439516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-01-06 09:29:44.439626: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-06 09:29:44.439695: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-06 09:29:44.439725: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-06 09:29:44.439753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-06 09:29:44.439781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-06 09:29:44.439808: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-06 09:29:44.439837: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-06 09:29:44.440727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-01-06 09:29:44.440781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-06 09:29:44.440791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2022-01-06 09:29:44.440801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2022-01-06 09:29:44.441691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 9983 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8db4bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/jdeke/anaconda3/envs/tf23/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "if language != \"All\":\n",
    "    df = pd.read_csv(\"/media/remote/jdeke/Dataframes/df_combined.csv\", header=0)\n",
    "    df = df[df[\"language key\"]==language][[\"short text\",\"material group\"]]\n",
    "else:\n",
    "    df = pd.read_csv(\"/media/remote/jdeke/Dataframes/df_combined.csv\", header=0)\n",
    "    df = df[[\"short text\",\"material group\", \"language key\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69cdac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short text\"] = df[\"short text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "335a2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('material group').filter(lambda x : len(x) > 1) # filter out classes occuring only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74296565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1FSCONMAT': 0,\n",
       " '1ROMACHIN': 1,\n",
       " '1ITSVINSW': 2,\n",
       " '1ITHWAPPL': 3,\n",
       " '1COPDPRNT': 4,\n",
       " '1FSMAINTE': 5,\n",
       " '1ITSWBUSI': 6,\n",
       " '1FMOSOSF': 7,\n",
       " '1FSBUEQMT': 8,\n",
       " '1LOGSERWC': 9,\n",
       " '1MSRMESCO': 10,\n",
       " '1ITSVSTSW': 11,\n",
       " '1ITTCVOMO': 12,\n",
       " '1FMREGARD': 13,\n",
       " '1TFLEGAL': 14,\n",
       " '1ROLOAD': 15,\n",
       " '1STAMAINT': 16,\n",
       " '1ITSWOFFI': 17,\n",
       " '1FSCIVILW': 18,\n",
       " '1CIVENGIN': 19,\n",
       " '1LOGSERSF': 20,\n",
       " '1FMOSCMSV': 21,\n",
       " '1HRTRANSL': 22,\n",
       " '1COATLCRE': 23,\n",
       " '1CICOHSSE': 24,\n",
       " '1ELCOMAIN': 25,\n",
       " '1HSSSFSSE': 26,\n",
       " '1FSITCOMM': 27,\n",
       " '1MSRLRS': 28,\n",
       " '1HSSSSPRS': 29,\n",
       " '1FSENGNRG': 30,\n",
       " '1STAEAIRC': 31,\n",
       " '1FMOSCLPS': 32,\n",
       " '1WSRSLABA': 33,\n",
       " '1FSBUPREF': 34,\n",
       " '1FSCARWAS': 35,\n",
       " '1LOGVEHCL': 36,\n",
       " '1FSFUELTE': 37,\n",
       " '1FSSIGNW': 38,\n",
       " '1FSFCEQMT': 39,\n",
       " '1ITSVINF': 40,\n",
       " '1MSRSSECU': 41,\n",
       " '1CICOMNGM': 42,\n",
       " '1HSENSTAU': 43,\n",
       " '1ITHWDESK': 44,\n",
       " '1FMREBLDM': 45,\n",
       " '1COPRPROM': 46,\n",
       " '1HRTLTL': 47,\n",
       " '1HRTROMV': 48,\n",
       " '1FMOSARCH': 49,\n",
       " '1ITSVCONS': 50,\n",
       " '1HRSEREAS': 51,\n",
       " '1PRSPWAST': 52,\n",
       " '1FMREFURN': 53,\n",
       " '1FMOSCNTS': 54,\n",
       " '1FMRENTAL': 55,\n",
       " '1COMEDIAB': 56,\n",
       " '1CICOAUCO': 57,\n",
       " '1LOGROANO': 58,\n",
       " '1AWAUXWO': 59,\n",
       " '1ITTCDAFI': 60,\n",
       " '1HSSSPPPE': 61,\n",
       " '1HSENWDMR': 62,\n",
       " '1CIVWORKS': 63,\n",
       " '1ENOWNER': 64,\n",
       " '1LOGVEHFB': 65,\n",
       " '1ITHWNOTE': 66,\n",
       " '1FSFURNIT': 67,\n",
       " '1CICOFICO': 68,\n",
       " '1CICOPERM': 69,\n",
       " '1LOGVEHCR': 70,\n",
       " '1LOGVEHCM': 71,\n",
       " '1CICOREES': 72,\n",
       " '1FMREBREN': 73,\n",
       " '1COBTLPRO': 74,\n",
       " '1GSMKIDI': 75,\n",
       " '1HRTRFSP': 76,\n",
       " '1PRWAWAST': 77,\n",
       " '1COBTLEVE': 78,\n",
       " '1CONTMATR': 79,\n",
       " '1CHXX': 80,\n",
       " '1UTILELEC': 81,\n",
       " '1UTILWATE': 82,\n",
       " '1UTILHEAT': 83,\n",
       " '1FMBMREN': 84,\n",
       " '1MSRSFIRE': 85,\n",
       " '1HSHHMEDC': 86,\n",
       " '1OSSENCON': 87,\n",
       " '1HSHHMEDS': 88,\n",
       " '1HSHHMEDE': 89,\n",
       " '1ENTECLIC': 90,\n",
       " '1ITTCSECU': 91,\n",
       " '1MSRSINST': 92,\n",
       " '1WSSEQCAS': 93,\n",
       " '1PIVAVALV': 94,\n",
       " '1WSOFHAIR': 95,\n",
       " '1ELROGEN': 96,\n",
       " '1LOGSERPY': 97,\n",
       " '1PRSESTMS': 98,\n",
       " '1WCSSPACK': 99,\n",
       " '1CICOPMS': 100,\n",
       " '1ENFEED': 101,\n",
       " '1OSCMDMSF': 102,\n",
       " '1MSRMAINT': 103,\n",
       " '1DRDEFMET': 104,\n",
       " '1ENCONCEP': 105,\n",
       " '1PROSMAIN': 106,\n",
       " '1PROOOPER': 107,\n",
       " '1ENDEXX': 108,\n",
       " '1TFNCI': 109,\n",
       " '1AWSCAFFO': 110,\n",
       " '1WSDSTRUN': 111,\n",
       " '1ROLIFTCR': 112,\n",
       " '1PRSMSCMS': 113,\n",
       " '1WCSSSERV': 114,\n",
       " '1ROPUM&R': 115,\n",
       " '1ELCOMAT': 116,\n",
       " '1ENPROMIN': 117,\n",
       " '1INISHEAT': 118,\n",
       " '1ROTUSPAR': 119,\n",
       " '1WCSSSRPU': 120,\n",
       " '1MSRDCS': 121,\n",
       " '1PIMAINT': 122,\n",
       " '1LOGVEHTM': 123,\n",
       " '1PRWAANNP': 124,\n",
       " '1ROPUROTA': 125,\n",
       " '1WSWEWSGE': 126,\n",
       " '1LOGVEHTS': 127,\n",
       " '1CONRAILC': 128,\n",
       " '1AWSITEDC': 129,\n",
       " '1MSRLAB': 130,\n",
       " '1PRSEOMAI': 131,\n",
       " '1STAEOTHE': 132,\n",
       " '1OSSHOR': 133,\n",
       " '1ROCOM&R': 134,\n",
       " '1DRDELHAN': 135,\n",
       " '1ROTUM&R': 136,\n",
       " '1PIFIHOSE': 137,\n",
       " '1PITCCONS': 138,\n",
       " '1CORPRINT': 139,\n",
       " '1CHGASBOT': 140,\n",
       " '1ROCOEQMT': 141,\n",
       " '1PRSMSLIC': 142,\n",
       " '1PICONSTR': 143,\n",
       " '1GSRECMGS': 144,\n",
       " '1PRSECTMS': 145,\n",
       " '1WSDSDRON': 146,\n",
       " '1ROLIFTCO': 147,\n",
       " '1DRDMCASI': 148,\n",
       " '1ENFEASIB': 149,\n",
       " '1CICOCOMM': 150,\n",
       " '1MSREQMT': 151,\n",
       " '1WSDSCODR': 152,\n",
       " '1DRDERUNT': 153,\n",
       " '1WSDSELWL': 154,\n",
       " '1DRDEWDET': 155,\n",
       " '1OSFLOAT': 156,\n",
       " '1PIFIMAIN': 157,\n",
       " '1WSDSCMNT': 158,\n",
       " '1DRDMPIPE': 159,\n",
       " '1HRSESURV': 160,\n",
       " '1WSDSWELT': 161,\n",
       " '1DRDMTUBI': 162,\n",
       " '1STAINSTA': 163,\n",
       " '1LOGVEHTR': 164,\n",
       " '1WSWEH2SS': 165,\n",
       " '1WSDSDFSC': 166,\n",
       " '1WSWEPERM': 167,\n",
       " '1DRDMBITS': 168,\n",
       " '1GBRMABI': 169,\n",
       " '1WSDSDROF': 170,\n",
       " '1COMEDIA': 171,\n",
       " '1WCSSESPU': 172,\n",
       " '1WSDSPERF': 173,\n",
       " '1CHCATHAN': 174,\n",
       " '1WSWEWELM': 175,\n",
       " '1PROSWKOV': 176,\n",
       " '1PRSEWIWO': 177,\n",
       " '1TRACHTON': 178,\n",
       " '1CHLABCHE': 179,\n",
       " '1WSSEACQP': 180,\n",
       " '1DRDEWDSP': 181,\n",
       " '1ITHWARE': 182,\n",
       " '1WSRSSTIN': 183,\n",
       " '1PIPLINSP': 184,\n",
       " '1WSDSDIRD': 185,\n",
       " '1DRDEWHEA': 186,\n",
       " '1COPRSERV': 187,\n",
       " '1LOGROAFC': 188,\n",
       " '1OSSENDD': 189,\n",
       " '1ROMATOOL': 190,\n",
       " '1HRLEGAL': 191,\n",
       " '1CHLUBES': 192,\n",
       " '1DRDMCEAD': 193,\n",
       " '1DRDEXTRE': 194,\n",
       " '1ROPUSPAR': 195,\n",
       " '1LOGROALP': 196,\n",
       " '1LOGSHIPT': 197,\n",
       " '1LOGROAFR': 198,\n",
       " '1FMOSRELO': 199,\n",
       " '1PIFIPIBU': 200,\n",
       " '1PREMPREM': 201,\n",
       " '1TRAGTRAV': 202,\n",
       " '1PIVASAFE': 203,\n",
       " '1CHWATWAS': 204,\n",
       " '1CHOILPRO': 205,\n",
       " '1UTILNGAS': 206,\n",
       " '1AWSTEELC': 207,\n",
       " '1WSWEQAQC': 208,\n",
       " '1CICOSTRA': 209,\n",
       " '1LOGVEHCB': 210,\n",
       " '1WSDSMLOG': 211,\n",
       " '1ITTCDATA': 212,\n",
       " '1OSUWAT': 213,\n",
       " '1PITCPISU': 214,\n",
       " '1LOGVEHCF': 215,\n",
       " '1HSENWWSR': 216,\n",
       " '1CONMATER': 217,\n",
       " '1HSENSTAS': 218,\n",
       " '1PIGPMSER': 219,\n",
       " '1COWEBCO': 220,\n",
       " '1STASPARE': 221,\n",
       " '1OSMOS': 222,\n",
       " '1CONEQUIP': 223,\n",
       " '1WSWEEMER': 224,\n",
       " '1WSWEWEPD': 225,\n",
       " '1ITTCVOIC': 226,\n",
       " '1PIFISPMA': 227,\n",
       " '1ITSVSOFT': 228,\n",
       " '1WSDSFISH': 229,\n",
       " '1HSENABDC': 230,\n",
       " '1LOGVEHFM': 231,\n",
       " '1TRACMREC': 232,\n",
       " '1LOGROALG': 233,\n",
       " '1TRTRREBU': 234,\n",
       " '1COPUBFEE': 235,\n",
       " '1CONTSANI': 236,\n",
       " '1AWHCV': 237,\n",
       " '1PREMWHC': 238,\n",
       " '1COEVENT': 239,\n",
       " '1STAEHEXC': 240,\n",
       " '1ENPROMAJ': 241,\n",
       " '1OSHECB': 242,\n",
       " '1STAECOLM': 243,\n",
       " '1AWCOAT': 244,\n",
       " 'LEGAL-ADV': 245,\n",
       " '1COADSERV': 246,\n",
       " '1COTRANSL': 247,\n",
       " 'MEM-FEES': 248,\n",
       " '1ITSVCLOU': 249,\n",
       " '1ELROELMO': 250,\n",
       " 'ADV-FIN': 251,\n",
       " 'ADV-REC': 252,\n",
       " 'FIN-AUDIT': 253,\n",
       " '1WSSEPRPI': 254,\n",
       " '1MERCATEO': 255,\n",
       " '1CONTPERS': 256,\n",
       " '1LOGVEHVR': 257,\n",
       " '1PIPILAUN': 258,\n",
       " '1WSDSCADR': 259,\n",
       " '1WSSEDAOF': 260,\n",
       " '1WSWELABS': 261,\n",
       " '1STAEFILT': 262,\n",
       " '1PIFIFBWF': 263,\n",
       " '1PILINEPI': 264,\n",
       " '1PREMOMAI': 265,\n",
       " '1WCSSSSCR': 266,\n",
       " '1ROCOSPAR': 267,\n",
       " '1DRDEWORI': 268,\n",
       " '1WCSSCOEQ': 269,\n",
       " '1OSSURF': 270,\n",
       " '1CHGLYCOL': 271,\n",
       " '1CROCRANE': 272,\n",
       " '1PROEMSOE': 273,\n",
       " '1WCSSPERF': 274,\n",
       " '1FSBUILD': 275,\n",
       " '1CORMARE': 276,\n",
       " '1LOGVEHTF': 277}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df[\"material group\"].unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "386f930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label_dict)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7f91666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df[\"material group\"].replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3976eb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short text</th>\n",
       "      <th>material group</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373366</th>\n",
       "      <td>hygienic audits for FS 2019 - Šediváková</td>\n",
       "      <td>1FSCONMAT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373367</th>\n",
       "      <td>Missing offers from Bronti</td>\n",
       "      <td>1ROMACHIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373368</th>\n",
       "      <td>Repair works of Thermooilpumps from Ferv</td>\n",
       "      <td>1ROMACHIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373369</th>\n",
       "      <td>Bronti January 2019</td>\n",
       "      <td>1ROMACHIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373370</th>\n",
       "      <td>Termoko service</td>\n",
       "      <td>1ROMACHIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      short text material group  label\n",
       "373366  hygienic audits for FS 2019 - Šediváková      1FSCONMAT      0\n",
       "373367                Missing offers from Bronti      1ROMACHIN      1\n",
       "373368  Repair works of Thermooilpumps from Ferv      1ROMACHIN      1\n",
       "373369                       Bronti January 2019      1ROMACHIN      1\n",
       "373370                           Termoko service      1ROMACHIN      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "769e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"short text\"].values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.2, \n",
    "                                                    random_state=seed,\n",
    "                                                    stratify=df[\"label\"], shuffle=True\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844159a",
   "metadata": {},
   "source": [
    "# End Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4a14d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output hidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "2022-01-06 09:29:51.500615: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='distilbert-base-multilingual-cased', vocab_size=119547, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel object at 0x7f51fc3fc910>\n"
     ]
    }
   ],
   "source": [
    "if layer_strategy != \"last\":\n",
    "    output_hidden_states = True\n",
    "    print('output hidden')\n",
    "else:\n",
    "    output_hidden_states = False\n",
    "\n",
    "    \n",
    "    \n",
    "if bert_type == \"BERT\":\n",
    "    if cased == False:\n",
    "        # Load transformers config and set output_hidden_states to False\n",
    "        if language != \"All\":\n",
    "\n",
    "            language_size_model_dict = {\"EN\": {\"specific\":'bert-base-uncased', \n",
    "                                           \"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"DE\":{\"specific\":\"bert-base-german-cased\", \n",
    "                                              \"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"RO\":{\"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"All\":{\"multi\":'bert-base-multilingual-cased'}}\n",
    "            \n",
    "            config = BertConfig.from_pretrained(language_size_model_dict[language][language_model_relation])\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained(language_size_model_dict[language][language_model_relation], config = config)\n",
    "            transformer_model = TFBertModel.from_pretrained(language_size_model_dict[language][language_model_relation], config = config)\n",
    "\n",
    "        else:\n",
    "            config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', config=config)\n",
    "            transformer_model = TFBertModel.from_pretrained('bert-base-multilingual-cased',config=config)\n",
    "\n",
    "    else:\n",
    "        if language != \"All\":\n",
    "\n",
    "            language_size_model_dict = {\"EN\": {\"specific\":'bert-base-cased', \n",
    "                                           \"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"DE\":{\"specific\":\"bert-base-german-cased\", \n",
    "                                              \"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"RO\":{\"multi\":'bert-base-multilingual-cased'}, \n",
    "                                        \"All\":{\"multi\":'bert-base-multilingual-cased'}}\n",
    "            config = BertConfig.from_pretrained(language_size_model_dict[language][language_model_relation])\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained(language_size_model_dict[language][language_model_relation], config=config)\n",
    "            transformer_model = TFBertModel.from_pretrained(language_size_model_dict[language][language_model_relation],config=config)\n",
    "\n",
    "        else:\n",
    "            config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', config=config)\n",
    "            transformer_model = TFBertModel.from_pretrained('bert-base-multilingual-cased',config=config)\n",
    "\n",
    "elif bert_type == \"Albert\":\n",
    "    size_model_dict = {\"small\":\"\",\"medium\":'albert-base-v2',\"large\":'albert-large-v2'}\n",
    "    \n",
    "    config = AlbertConfig.from_pretrained(size_model_dict[size])\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(size_model_dict[size], config=config)\n",
    "    transformer_model = TFAlbertModel.from_pretrained(size_model_dict[size],config=config)\n",
    "\n",
    "elif bert_type == \"Roberta\":\n",
    "    size_model_dict = {\"small\":'roberta-base',\"medium\":'roberta-base',\"large\":'roberta-large'}\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(size_model_dict[size])\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(size_model_dict[size],config=config)\n",
    "    transformer_model = TFRobertaModel.from_pretrained(size_model_dict[size],config=config)\n",
    "    \n",
    "elif bert_type == \"RobertaXLM\":\n",
    "    config = XLMRobertaConfig.from_pretrained('jplu/tf-xlm-roberta-base')\n",
    "    config.output_hidden_states = output_hidden_states\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained('jplu/tf-xlm-roberta-base', config=config)\n",
    "    transformer_model = TFXLMRobertaModel.from_pretrained('jplu/tf-xlm-roberta-base', config=config)\n",
    "\n",
    "elif bert_type == \"Distilbert\":\n",
    "    if cased == False:\n",
    "        if language != \"All\":\n",
    "\n",
    "            language_size_model_dict = {\"EN\": {\"specific\":'distilbert-base-uncased', \n",
    "                                           \"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"DE\":{\"specific\":\"distilbert-base-german-cased\", \n",
    "                                              \"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"RO\":{\"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"All\":{\"multi\":'distilbert-base-multilingual-cased'}}\n",
    "            \n",
    "            config = DistilBertConfig.from_pretrained(language_size_model_dict[language][language_model_relation])\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained(language_size_model_dict[language][language_model_relation], config=config)\n",
    "            transformer_model = TFDistilBertModel.from_pretrained(language_size_model_dict[language][language_model_relation],config=config)\n",
    "\n",
    "        else:\n",
    "            config = DistilBertConfig.from_pretrained('distilbert-base-multilingual-cased')\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased',config=config)\n",
    "            transformer_model = TFDistilBertModel.from_pretrained('bert-base-multilingual-cased',config=config)\n",
    "\n",
    "    else:\n",
    "        if language != \"All\":\n",
    "\n",
    "            language_size_model_dict = {\"EN\": {\"specific\":'distilbert-base-cased', \n",
    "                                           \"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"DE\":{\"specific\":\"distilbert-base-german-cased\", \n",
    "                                              \"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"RO\":{\"multi\":'distilbert-base-multilingual-cased'}, \n",
    "                                        \"All\":{\"multi\":'distilbert-base-multilingual-cased'}}\n",
    "            config = DistilBertConfig.from_pretrained(language_size_model_dict[language][language_model_relation])\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained(language_size_model_dict[language][language_model_relation], config=config)\n",
    "            transformer_model = TFDistilBertModel.from_pretrained(language_size_model_dict[language][language_model_relation],config=config)\n",
    "\n",
    "        else:\n",
    "            config = DistilBertConfig.from_pretrained('distilbert-base-multilingual-cased')\n",
    "            config.output_hidden_states = output_hidden_states\n",
    "            tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased', config=config)\n",
    "            transformer_model = TFDistilBertModel.from_pretrained('bert-base-multilingual-cased', config=config)\n",
    "            \n",
    "print(tokenizer)\n",
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac9beeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b65d3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### ------- Build the model ------- ###\n",
    "#implementation according to transformer library --> original setup as in paper\n",
    "\n",
    "reduce = tf.math.reduce_mean if reduce_strategy=='mean' else tf.math.reduce_max\n",
    "\n",
    "def build_classifier_model(training=training):\n",
    "    transformer = transformer_model.layers[0]\n",
    "    # Build your model input\n",
    "    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "    attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    if bert_type==\"Roberta\":\n",
    "        roberta_model = transformer(inputs, training=training)[0]#sequence output because pooler output not good for roberta\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = roberta_model[:,0,:]\n",
    "        else:\n",
    "            pooled_output = reduce(roberta_model,axis=1)\n",
    "        #basically what follows is what is done to compute pooled output\n",
    "        dropout_pooler = Dropout(config.hidden_dropout_prob, name='pooler_dropout', seed=seed)\n",
    "        pooled_output_dropout = dropout_pooler(pooled_output, training=training)\n",
    "        hidden_roberta = Dense(units=config.hidden_size, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='tanh')(pooled_output_dropout) #take s token\n",
    "        #from here pooled output is used essentially\n",
    "        dropout_hidden = Dropout(config.hidden_dropout_prob, name='hidden_output', seed=seed)\n",
    "        hidden_output_roberta = dropout_hidden(hidden_roberta, training=training)\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(hidden_output_roberta)\n",
    "        \n",
    "    elif bert_type==\"Distilbert\":\n",
    "        distilbert_model = transformer(inputs, training=training)[0]\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = distilbert_model[:,0]\n",
    "        else:\n",
    "            pooled_output = reduce(distilbert_model, axis=1)\n",
    "        #for distilbert, the pooler is a bit different, e.g. relu activation\n",
    "        hidden_layer = Dense(units=config.dim, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='relu')(pooled_output)\n",
    "        dropout_hidden = Dropout(config.seq_classif_dropout, name='hidden_output', seed=seed)\n",
    "        hidden_output_distilbert = dropout_hidden(hidden_layer, training=training)\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(hidden_output_distilbert)\n",
    "        \n",
    "    else:\n",
    "        # Load the Transformers BERT model as a layer in a Keras model\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = transformer(inputs, training=training)[1] #pooler output in tensorflow\n",
    "        else:\n",
    "            bert_model = transformer(inputs, training=training)[0] #pooler output in tensorflow\n",
    "            pooled_output = reduce(bert_model,axis=1)\n",
    "            dropout_pooler = Dropout(config.hidden_dropout_prob, name='pooler_dropout', seed=seed)\n",
    "            pooled_output = dropout_pooler(pooled_output, training=training)\n",
    "            pooled_output = Dense(units=config.hidden_size, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='tanh')(pooled_output) #take s token\n",
    "        # Then build your model output\n",
    "        dropout = Dropout(config.hidden_dropout_prob, name='pooled_output', seed=seed)\n",
    "        pooled_output = dropout(pooled_output, training=training) #training=False\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(pooled_output)\n",
    "            \n",
    "    #outputs = {'material': material}\n",
    "    # And combine it all in a model object\n",
    "    model = Model(inputs=inputs, outputs=material, name='BERT_MultiClass')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a04c53",
   "metadata": {},
   "source": [
    "# Last x layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c1bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last4 concat strategy either cls, mean or max of respective hidden state\n",
    "\n",
    "reduce = tf.math.reduce_mean if reduce_strategy=='mean' else tf.math.reduce_max\n",
    "reduce_cross_layer = tf.math.reduce_mean if last_layer_strategy=='mean' else tf.math.reduce_max\n",
    "\n",
    "def build_classifier_model_last4(training=training):\n",
    "    transformer = transformer_model.layers[0]\n",
    "    # Build your model input\n",
    "    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "    attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    if bert_type==\"Roberta\":\n",
    "        roberta_model = transformer(inputs, training=training)[2]#models hidden states\n",
    "        if last_layer_strategy==\"concat\":\n",
    "            pooled_output = tf.concat(tuple([roberta_model[i] for i in [-4, -3, -2, -1]]), axis=-1) #4 last hidden states\n",
    "        else:\n",
    "            pooled_output = reduce_cross_layer(tuple([roberta_model[i] for i in [-4, -3, -2, -1]]), axis=0) #4 last hidden states\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "        else:\n",
    "            pooled_output = reduce(pooled_output,axis=1)\n",
    "        dropout_pooler = Dropout(config.hidden_dropout_prob, name='pooler_dropout', seed=seed)\n",
    "        pooled_output = dropout_pooler(pooled_output, training=training)\n",
    "        pooled_output = Dense(units=config.hidden_size, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='tanh')(pooled_output) #take s token\n",
    "        dropout_hidden = Dropout(config.hidden_dropout_prob, name='hidden_output', seed=seed)\n",
    "        pooled_output = dropout_hidden(pooled_output, training=training)\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(pooled_output)\n",
    "        \n",
    "    elif bert_type==\"Distilbert\":\n",
    "        distilbert_model = transformer(inputs, training=training)[1]\n",
    "        if last_layer_strategy==\"concat\":\n",
    "            pooled_output = tf.concat(tuple([distilbert_model[i] for i in [-2, -1]]), axis=-1)\n",
    "        else:\n",
    "            pooled_output = reduce_cross_layer(tuple([distilbert_model[i] for i in [-2, -1]]), axis=0)\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "        else:\n",
    "            pooled_output = reduce(pooled_output,axis=1)\n",
    "        #no dropout here because pooler is not using dropout in distilbert\n",
    "        #dropout_pooler = Dropout(config.seq_classif_dropout, name='pooler_dropout', seed=seed)\n",
    "        #pooled_output = dropout_pooler(pooled_output)\n",
    "        pooled_output = Dense(units=config.dim, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='relu')(pooled_output)\n",
    "        dropout_hidden = Dropout(config.seq_classif_dropout, name='hidden_output', seed=seed)\n",
    "        pooled_output = dropout_hidden(pooled_output, training=training)\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(pooled_output)\n",
    "        \n",
    "    else:\n",
    "    # Load the Transformers BERT model as a layer in a Keras model\n",
    "        bert_hidden_states = transformer(inputs, training=training)[2] #hidden states\n",
    "        # Then build your model output\n",
    "        if last_layer_strategy==\"concat\":\n",
    "            pooled_output = tf.concat(tuple([bert_hidden_states[i] for i in [-4, -3, -2, -1]]), axis=-1)\n",
    "        else:\n",
    "            pooled_output = reduce_cross_layer(tuple([bert_hidden_states[i] for i in [-4, -3, -2, -1]]), axis=0)\n",
    "        if reduce_strategy == 'cls':\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "        else:\n",
    "            pooled_output = reduce(pooled_output,axis=1)\n",
    "        dropout = Dropout(config.hidden_dropout_prob, name='pooled_output', seed=seed)\n",
    "        pooled_output = dropout(pooled_output, training=training) \n",
    "        pooled_output = Dense(units=config.hidden_size, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='hidden', activation='relu')(pooled_output)\n",
    "        dropout_hidden = Dropout(config.hidden_dropout_prob, name='hidden_output', seed=seed)\n",
    "        pooled_output = dropout_hidden(pooled_output, training=training)\n",
    "        material = Dense(units=len(label_dict), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='material')(pooled_output)\n",
    "\n",
    "    #outputs = {'material': material}\n",
    "    # And combine it all in a model object\n",
    "    model = Model(inputs=inputs, outputs=material, name='BERT_MultiClass')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae45a9",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd6fad92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Strategy is: last4\n",
      "Reduce Strategy is: cls\n",
      "Model: \"BERT_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask (InputLayer)     [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distilbert (TFDistilBertMainLay TFBaseModelOutput(la 134734080   attention_mask[0][0]             \n",
      "                                                                 input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 40, 1536)]   0           distilbert[0][5]                 \n",
      "                                                                 distilbert[0][7]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1536)]       0           tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 768)          1180416     tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hidden_output (Dropout)         (None, 768)          0           hidden[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "material (Dense)                (None, 278)          213782      hidden_output[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 136,128,278\n",
      "Trainable params: 136,128,278\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer Strategy is:\",layer_strategy)\n",
    "print(\"Reduce Strategy is:\", reduce_strategy)\n",
    "\n",
    "# Take a look at the model\n",
    "if layer_strategy == 'last':\n",
    "    model = build_classifier_model(training=training)\n",
    "elif layer_strategy == 'last4':\n",
    "    model = build_classifier_model_last4(training=training)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53541e63",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11dffd",
   "metadata": {},
   "source": [
    "# Calibration Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d0ad8d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f523c6a6be0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for vanilla BERT\n",
    "model.load_weights('models/fit/EN_last4_concat_cls_1_multi_False_Distilbert_0.0001_5_20220104-233911_random_state42')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "190576d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### ----- Evaluate the model ------ ###\n",
    "# Ready test data\n",
    "test_y_material = to_categorical(y_test,num_classes=n_classes)\n",
    "test_x = tokenizer(\n",
    "    text=list(X_test),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0f59e",
   "metadata": {},
   "source": [
    "# Time Taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b181f",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fccf32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_sampling(test_examples):\n",
    "  # Enable dropout during inference.\n",
    "  return model.predict(x={'input_ids': test_x['input_ids'],'attention_mask':test_x[\"attention_mask\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.79034161567688\n",
      "0.0025994378738435527\n"
     ]
    }
   ],
   "source": [
    "ensembles = [3,5,10,20,30,40,50,60,70,80,90,100] #3,5,10,20,30,40,50,60,70,80,90,100\n",
    "metrics_dict = defaultdict(dict)\n",
    "for num_ensemble in ensembles: \n",
    "    \n",
    "    # Take a look at the model\n",
    "    if layer_strategy == 'last':\n",
    "        model = build_classifier_model(training=training)\n",
    "    elif layer_strategy == 'last4':\n",
    "        model = build_classifier_model_last4(training=training)\n",
    "    \n",
    "    #for vanilla BERT\n",
    "    model.load_weights('models/fit/EN_last4_concat_cls_1_multi_False_Distilbert_0.0001_5_20220104-233911_random_state42')\n",
    "\n",
    "    start_time = time.time()\n",
    "    dropout_logit_samples = [model.predict(x={'input_ids': test_x['input_ids'],'attention_mask':test_x[\"attention_mask\"]}) for _ in range(num_ensemble)]\n",
    "    dropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1) for dropout_logits in dropout_logit_samples]\n",
    "    dropout_prob_samples = tf.stack([dropout_prob_samples])[0]\n",
    "    dropout_probs_mean = tf.reduce_mean(dropout_prob_samples, axis=0)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    time_elapsed_per_example = time_elapsed/y_test.shape[0]\n",
    "    \n",
    "    metrics_dict[num_ensemble][\"time_elapsed\"] = time_elapsed\n",
    "    metrics_dict[num_ensemble][\"time_elapsed_per_example\"] = time_elapsed_per_example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(time_elapsed)\n",
    "    print(time_elapsed_per_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data=metrics_dict, orient='index').to_csv(\"DE_\"+str(seed) + '_MC Dropout time.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a302f5",
   "metadata": {},
   "source": [
    "Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf24f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.23747539520264\n",
      "0.003345110602704857\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = defaultdict(dict)\n",
    "start_time = time.time()\n",
    "\n",
    "# Take a look at the model\n",
    "if layer_strategy == 'last':\n",
    "    model = build_classifier_model(training=training)\n",
    "elif layer_strategy == 'last4':\n",
    "    model = build_classifier_model_last4(training=training)\n",
    "\n",
    "#for vanilla BERT\n",
    "model.load_weights('models/fit/DE_last4_concat_mean_1_specific_False_BERT_0.0001_5_20211103-100737_random_state99')\n",
    "dropout_logit_samples = model.predict(x={'input_ids': test_x['input_ids'],'attention_mask':test_x[\"attention_mask\"]})\n",
    "dropout_prob_samples = tf.nn.softmax(dropout_logit_samples)\n",
    "end_time = time.time()\n",
    "\n",
    "time_elapsed = (end_time - start_time)\n",
    "\n",
    "time_elapsed_per_example = time_elapsed/y_test.shape[0]\n",
    "\n",
    "metrics_dict[\"1\"][\"time_elapsed\"] = time_elapsed\n",
    "metrics_dict[\"1\"][\"time_elapsed_per_example\"] = time_elapsed_per_example\n",
    "\n",
    "pd.DataFrame.from_dict(data=metrics_dict, orient='index').to_csv(\"DE_\"+ str(seed) + '_Vanilla time_more.csv', header=True)\n",
    "\n",
    "print(time_elapsed)\n",
    "print(time_elapsed_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108dc6c",
   "metadata": {},
   "source": [
    "Deep Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RO_last4_concat_cls_1_multi_False_Distilbert_0.0001_5_20211021-073036_random_state42\n"
     ]
    }
   ],
   "source": [
    "ensembles = [3,4,5,6,7,8,9,10]\n",
    "metrics_dict = defaultdict(dict)\n",
    "x = glob.glob(\"/media/remote/jdeke/models/fit/*.index\")\n",
    "\n",
    "start_time = time.time()\n",
    "for num_ensemble in ensembles:\n",
    "    model_logit_samples = []\n",
    "    for f in x[:num_ensemble]:\n",
    "\n",
    "        print(os.path.basename(f)[:-6])\n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model()\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4()\n",
    "\n",
    "        model.load_weights('models/fit/'+os.path.basename(f)[:-6])\n",
    "        model_logit_samples.append(model.predict(x={'input_ids': test_x['input_ids'],'attention_mask':test_x[\"attention_mask\"]}))\n",
    "        \n",
    "    dropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1) for dropout_logits in model_logit_samples]\n",
    "    dropout_prob_samples = tf.stack([dropout_prob_samples])[0]\n",
    "    dropout_probs_mean = tf.reduce_mean(dropout_prob_samples, axis=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_elapsed = (end_time - start_time)\n",
    "\n",
    "    time_elapsed_per_example = time_elapsed/y_test.shape[0]\n",
    "    \n",
    "    metrics_dict[num_ensemble][\"time_elapsed\"] = time_elapsed\n",
    "    metrics_dict[num_ensemble][\"time_elapsed_per_example\"] = time_elapsed_per_example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(time_elapsed)\n",
    "    print(time_elapsed_per_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data=metrics_dict, orient='index').to_csv(\"RO_\"+str(seed) + '_Deep Ensemble time.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf23] *",
   "language": "python",
   "name": "conda-env-tf23-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
