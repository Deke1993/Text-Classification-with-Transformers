{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a63572",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0911cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:12:31.091675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "### -------- Load libraries ------- ###\n",
    "# Load Huggingface transformers\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizer\n",
    "from transformers import TFAlbertModel,  AlbertConfig, AlbertTokenizer\n",
    "from transformers import TFRobertaModel,  RobertaConfig, RobertaTokenizer\n",
    "from transformers import TFDistilBertModel, BertTokenizer, DistilBertConfig\n",
    "from transformers import TFXLMModel, XLMTokenizer, XLMConfig, TFSequenceSummary\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "# tensorflow.\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import official.nlp.modeling.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "# others\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "#loggging\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#custom functions\n",
    "from Refactoring.loading import load, load_combined_languages, load_artificial_ood\n",
    "from Refactoring.model import build_classifier_model, build_classifier_model_last4, load_model\n",
    "from Refactoring.model import bert_optimizer, get_layers, dlr_optimizer\n",
    "from Refactoring.evaluation import test_prediction, save_report, save_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1cf78",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85cf7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf seeds set\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "######### Necessary Setting s########\n",
    "\n",
    "\n",
    "train_part2 = False #if one wants to train models without setting seeds for the model, set this to \"True\". \n",
    "                    #Necessary for Deep Ensembles & MCD\n",
    "    \n",
    "train_part2_ood = False #set to true if one wants to train for artificial ood setting --> different dataset loaded. \n",
    "                        #separate evaluation script\n",
    "                        #if activated, also \"train_part2\" should be True\n",
    "\n",
    "if train_part2_ood:\n",
    "    assert(train_part2==True)\n",
    "\n",
    "    \n",
    "n_models = 1 #models to be trained in loop, necessary for deep ensembles\n",
    "\n",
    "#####################################\n",
    "########## Seeds ####################\n",
    "seed= 42 #42 standard, 0, 21, 99, 365\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "if train_part2:\n",
    "    print(\"tf seeds not set\")\n",
    "    pass\n",
    "else:\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    \n",
    "    print(\"tf seeds set\")\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "######### Necessary Settings ########\n",
    "\n",
    "language = \"RO\" #EN, DE, RO, All \n",
    "\n",
    "language_model_relation = \"multi\" # specific vs multi\n",
    "\n",
    "bert_type = \"BERT\"  #Roberta, #Distilbert, #BERT # RobertaXLM\n",
    "\n",
    "epochs = 5 \n",
    "\n",
    "learning_rate = 1e-4 #5e-5 or 2e-5, 1e-4, 4e-4\n",
    "\n",
    "layer_strategy = 'last' #last, last4\n",
    "\n",
    "reduce_strategy = 'cls' # cls,mean, max\n",
    "\n",
    "last_layer_strategy = \"concat\" #mean, max, concat --> only relevant for last4 layer strategy\n",
    "\n",
    "decay_factor = 1 # only takes effect with layer wise lr\n",
    "\n",
    "layer_wise_lr = False #True for discirminative learning rates\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "#########  Default Settings  ########\n",
    "\n",
    "#these were not changed in the end. sometimes still used for logging/naming purposes\n",
    "\n",
    "size = \"medium\" #used only medium in the end\n",
    "\n",
    "cased = False # always used recommended case for each model, thus do not change\n",
    "\n",
    "class_weighting = False #relic of initial experiments\n",
    "\n",
    "max_length = 40 #only 40 used\n",
    "\n",
    "batch_size = 32 #only 32 used\n",
    "\n",
    "val_size=0.2 #only 0.2 used\n",
    "\n",
    "freeze = False #if one wants to train classification head first only. placeholder is in training loop, \n",
    "                #but should be adjusted if one wants to use it \n",
    "                #(e.g. set different learning rates for the two training phases)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69804f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tensorflow.python.client import device_lib\\nprint(device_lib.list_local_devices())\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check device\n",
    "'''\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66175e",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dd26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Classes Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3905431/3473985755.py:6: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  X_train, y_train, X_train_de, X_test_de, y_train_de, y_test_de, X_train_en, X_test_en, y_train_en, y_test_en, X_train_ro, X_test_ro, y_train_ro, y_test_ro, n_classes, label_dict = load_combined_languages(seed=seed)\n"
     ]
    }
   ],
   "source": [
    "if train_part2_ood == False: #if we do not delete certain classes from training set\n",
    "    print(\"No Classes Deleted\")\n",
    "    if language != \"All\": #if we only load test sets for one language\n",
    "        X_train, X_test, y_train, y_test, n_classes, label_dict = load(language=language, seed=seed)\n",
    "    else: #if we train on all and test on individual datasets\n",
    "        X_train, y_train, X_train_de, X_test_de, y_train_de, y_test_de, X_train_en, X_test_en, y_train_en, y_test_en, X_train_ro, X_test_ro, y_train_ro, y_test_ro, n_classes, label_dict = load_combined_languages(seed=seed)\n",
    "else: #if we delete certain classes\n",
    "    print(\"Some Classes Deleted\")\n",
    "    X_train, X_test, y_train, y_test, n_classes, label_dict = load_artificial_ood(language=language, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c033e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(X_train) #needed for the optimizer, which is building learning rate schedule depending on number of training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6e419",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c34a1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9507f219e69f45238511f8ecbd16f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:14:45.933176: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-07 14:14:45.943778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-01-07 14:14:45.943848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:14:45.949132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-07 14:14:45.952891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-07 14:14:45.954673: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-07 14:14:45.958642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-07 14:14:45.961311: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-07 14:14:45.968435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-07 14:14:45.970101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-01-07 14:14:45.970688: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 14:14:45.992098: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3850000000 Hz\n",
      "2022-01-07 14:14:45.993136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c79436b0f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-07 14:14:45.993172: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-01-07 14:14:46.057350: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c7965b14d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-07 14:14:46.057391: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2022-01-07 14:14:46.058522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-01-07 14:14:46.058578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:14:46.058620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-07 14:14:46.058642: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-07 14:14:46.058664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-07 14:14:46.058686: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-07 14:14:46.058709: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-07 14:14:46.058731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-07 14:14:46.060349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-01-07 14:14:46.060399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-07 14:14:46.525335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-07 14:14:46.525362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2022-01-07 14:14:46.525366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2022-01-07 14:14:46.526103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9511 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\n",
      "2022-01-07 14:14:46.712990: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "Some layers from the model checkpoint at jplu/tf-xlm-roberta-base were not used when initializing TFXLMRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='jplu/tf-xlm-roberta-base', vocab_size=250002, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "<transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaModel object at 0x7fbcc9e82f10>\n",
      "Layer Strategy is: last\n",
      "Reduce Strategy is: cls\n",
      "training with seeds\n",
      "Model: \"Text-Classifier\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask (InputLayer)     [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 278043648   attention_mask[0][0]             \n",
      "                                                                 input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pooled_output (Dropout)         (None, 768)          0           roberta[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 389)          299141      pooled_output[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 278,342,789\n",
      "Trainable params: 278,342,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "BERT\n",
      "<official.nlp.optimization.AdamWeightDecay object at 0x7fbcc8105e80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:15:01.452929: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2022-01-07 14:15:01.452971: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\n",
      "2022-01-07 14:15:01.453217: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-01-07 14:15:01.453283: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory\n",
      "2022-01-07 14:15:01.453292: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1/8651 [..............................] - ETA: 0s - loss: 5.9754 - accuracy: 0.0000e+00 - top2: 0.0000e+00 - top3: 0.0000e+00 - top4: 0.0000e+00 - top5: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:15:31.431948: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2022-01-07 14:15:31.431991: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   2/8651 [..............................] - ETA: 39:28 - loss: 5.9948 - accuracy: 0.0000e+00 - top2: 0.0000e+00 - top3: 0.0000e+00 - top4: 0.0000e+00 - top5: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:15:31.836145: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-01-07 14:15:31.865165: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31\n",
      "2022-01-07 14:15:31.875433: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.trace.json.gz\n",
      "2022-01-07 14:15:31.920673: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31\n",
      "2022-01-07 14:15:31.931897: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.memory_profile.json.gz\n",
      "2022-01-07 14:15:31.969576: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31Dumped tool data for xplane.pb to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/All_last_concat_cls_1_multi_False_RobertaXLM_0.0001_5_20220107-141501_random_state42/train/plugins/profile/2022_01_07_14_15_31/zwinge.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62/8651 [..............................] - ETA: 52:31 - loss: 5.9755 - accuracy: 0.0116 - top2: 0.0212 - top3: 0.0282 - top4: 0.0338 - top5: 0.0378"
     ]
    }
   ],
   "source": [
    "#load tokenizer, transformer_model, config\n",
    "tokenizer, transformer_model, config = load_model(layer_strategy=layer_strategy, bert_type=bert_type, cased=cased, language=language, language_model_relation=language_model_relation)\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(\"Layer Strategy is:\",layer_strategy)\n",
    "    print(\"Reduce Strategy is:\", reduce_strategy)\n",
    "\n",
    "    if train_part2: #if we use deep ensembles we cannot use seeds when building the model because the ensemble members do not find many different local minima\n",
    "        print(\"training without seeds\")\n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes)    \n",
    "    else:\n",
    "        print(\"training with seeds\")\n",
    "        if layer_strategy == 'last':\n",
    "            model = build_classifier_model(seed=seed, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes)\n",
    "        elif layer_strategy == 'last4':\n",
    "            model = build_classifier_model_last4(seed=seed, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    layer_list = get_layers(model) #in case one uses the opimtizer with DLR\n",
    "\n",
    "    optimizers_and_layers = dlr_optimizer(learning_rate, layer_list, decay_factor, batch_size, epochs, len_train, val_size) #build dlr optimizer\n",
    "\n",
    "    logdir=\"logs/fit/\" +str(language) +\"_\"+str(layer_strategy)+ \"_\"+str(last_layer_strategy) +\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor) + \"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +\"_\"+ str(epochs) +\"_\" +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed)\n",
    "    modeldir = \"models/fit/\" + str(language) +\"_\"+str(layer_strategy) + \"_\"+str(last_layer_strategy)+\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor)+\"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +\"_\"+str(epochs) +\"_\" +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed) \n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=modeldir,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(str(language) +\"_\"+str(layer_strategy)+ \"_\"+str(last_layer_strategy) +\"_\"+ str(reduce_strategy) + \"_\"+str(decay_factor) + \"_\" + str(language_model_relation) + \"_\" + str(cased)+ \"_\" + str(bert_type) +\"_\" + str(learning_rate) +\"_\"+str(epochs) +\"_\" +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+ \"_random_state\" + str(seed)+'.log')\n",
    "\n",
    "    #please note that one may/should use different learning rates in the two different training steps when using freezing; \n",
    "    if freeze:\n",
    "    # if settings indicate that classifier should be first trained isolatedly and only after that fine-tuning should start\n",
    "        model.layers[2].trainable=False #first set main model layer to non-trainable\n",
    "        optimizer = bert_optimizer(learning_rate, batch_size, epochs, len_train)\n",
    "\n",
    "        # Ready output data for the model\n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "\n",
    "        # Tokenize the input (takes some time)\n",
    "        x = tokenizer(\n",
    "        text=list(X_train),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True, \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True)\n",
    "\n",
    "        # Set loss and metrics\n",
    "        loss = {'output': CategoricalCrossentropy(from_logits = True)}\n",
    "        metric = {'output': [CategoricalAccuracy('accuracy'),tf.keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top2\"),tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3\"), tf.keras.metrics.TopKCategoricalAccuracy(k=4, name=\"top4\"),tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5\")]}\n",
    "\n",
    "\n",
    "        # Compile the model \n",
    "        model.compile(\n",
    "          optimizer = optimizer,\n",
    "          loss = loss, \n",
    "          metrics = metric)\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "          x={'input_ids': x['input_ids'], 'attention_mask':x[\"attention_mask\"]},\n",
    "          y=y_train_categorical,\n",
    "          validation_split=val_size,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, shuffle=True, callbacks=[tensorboard_callback,model_checkpoint_callback, csv_logger]) \n",
    "\n",
    "        model.layers[2].trainable=True #now we need to make base model trainable again and then compile again\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = loss, \n",
    "        metrics = metric)\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "        x={'input_ids': x['input_ids'], 'attention_mask':x[\"attention_mask\"]},\n",
    "        y=y_train_categorical,\n",
    "        validation_split=val_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, callbacks=[tensorboard_callback, csv_logger,model_checkpoint_callback], shuffle=True)\n",
    "\n",
    "    #######################################\n",
    "    ### ------- Train the model ------- ###\n",
    "    # Set optimizer # \n",
    "\n",
    "    if layer_wise_lr:\n",
    "        optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "    else:\n",
    "        optimizer = bert_optimizer(learning_rate, batch_size, epochs, len_train)\n",
    "\n",
    "    print(optimizer)\n",
    "\n",
    "    # Set loss and metrics\n",
    "    loss = {'output': CategoricalCrossentropy(from_logits = True)}\n",
    "    metric = {'output': [CategoricalAccuracy('accuracy'),tf.keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top2\"),tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3\"), tf.keras.metrics.TopKCategoricalAccuracy(k=4, name=\"top4\"),tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5\")]}\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = loss, \n",
    "        metrics = metric)\n",
    "\n",
    "    # Ready output data for the model\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "\n",
    "    # Tokenize the input\n",
    "    x = tokenizer(\n",
    "        text=list(X_train),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        x={'input_ids': x['input_ids'], 'attention_mask':x[\"attention_mask\"]},\n",
    "        y=y_train_categorical,\n",
    "        validation_split=val_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, callbacks=[tensorboard_callback, csv_logger,model_checkpoint_callback], shuffle=True) #,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a1bca",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set - only after hyperparameter tuning & only for in-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df420b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, transformer_model, config = load_model(layer_strategy=layer_strategy, bert_type=bert_type, cased=cased, language=language, language_model_relation=language_model_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30702373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer Strategy is:\",layer_strategy)\n",
    "print(\"Reduce Strategy is:\", reduce_strategy)\n",
    "\n",
    "if train_part2:\n",
    "    print(\"training without seeds\")\n",
    "    # Take a look at the model\n",
    "    if layer_strategy == 'last':\n",
    "        model = build_classifier_model(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes)\n",
    "    elif layer_strategy == 'last4':\n",
    "        model = build_classifier_model_last4(seed=None, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes)    \n",
    "else:\n",
    "    print(\"training with seeds\")\n",
    "    # Take a look at the model\n",
    "    if layer_strategy == 'last':\n",
    "        model = build_classifier_model(seed=seed, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, config=config, n_classes=n_classes)\n",
    "    elif layer_strategy == 'last4':\n",
    "        model = build_classifier_model_last4(seed=seed, tokenizer=tokenizer, transformer_model=transformer_model, max_length=max_length,bert_type=bert_type, reduce_strategy=reduce_strategy, last_layer_strategy=last_layer_strategy, config=config, n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir= \"EN_last4_concat_cls_1_specific_False_Distilbert_0.0001_5_20220104-185410_random_state42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a040fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model.load_weights('models/fit/'+modeldir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_label_dict = {v: k for k, v in label_dict.items()} #reverse the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb59552",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_part2_ood == False)\n",
    "if language != \"All\":\n",
    "    y_pred, predictions_logit, y_test_categorical = test_prediction(model,tokenizer, X_test, y_test, n_classes, max_length)\n",
    "    save_report(y_test,y_pred,n_classes,label_dict,inv_label_dict, language, layer_strategy, last_layer_strategy, reduce_strategy, decay_factor, language_model_relation, cased, bert_type, learning_rate, epochs, seed, test_lang=language)\n",
    "    save_metrics(y_test_categorical, predictions_logit,n_classes,label_dict,inv_label_dict, language, layer_strategy, last_layer_strategy, reduce_strategy, decay_factor, language_model_relation, cased, bert_type, learning_rate, epochs, seed, test_lang=language)\n",
    "else: #we need to evaluate the multilingual model on each language\n",
    "    for i in [[X_test_en, y_test_en,\"en\"],[X_test_de, y_test_de,\"de\"],[X_test_ro, y_test_ro,\"ro\"]]:\n",
    "        print(\"Evaluating the following language: \" + i[2])\n",
    "        y_pred, predictions_logit, y_test_categorical = test_prediction(model,tokenizer, i[0], i[1], n_classes, max_length)\n",
    "        save_report(i[1],y_pred,n_classes,label_dict,inv_label_dict, language, layer_strategy, last_layer_strategy, reduce_strategy, decay_factor, language_model_relation, cased, bert_type, learning_rate, epochs, seed, test_lang=i[2])\n",
    "        save_metrics(y_test_categorical, predictions_logit,n_classes,label_dict,inv_label_dict, language, layer_strategy, last_layer_strategy, reduce_strategy, decay_factor, language_model_relation, cased, bert_type, learning_rate, epochs, seed, test_lang=i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dae0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yourenv]",
   "language": "python",
   "name": "conda-env-yourenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
